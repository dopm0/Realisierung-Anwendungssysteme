import asyncio
import asyncpg
import requests
import pandas as pd
from datetime import datetime, timedelta
import json
from zipfile import ZipFile
from io import BytesIO

# === KONFIGURATION ===
FILTER_ID = 4169
REGION = "DE"
RESOLUTION = "quarterhour"
ZEITRAUM = 365
ZUKUNFT = 1

DB_USER = "master2025"
DB_PASS = "anwendungssysteme"
DB_HOST = "db.kaidro.de"
DB_PORT = "5432"
DB_NAME = "postgres"
DB_SCHEMA = "mw212_projekt"
TABLE_NAME = "Electricity_Price_History"


async def main():
    # Zeitraum berechnen
    end_date = datetime.now() + timedelta(days=ZUKUNFT)
    start_date = end_date - timedelta(days=ZEITRAUM)
    start_ts = int(start_date.timestamp() * 1000)
    end_ts = int(end_date.timestamp() * 1000)

    # === 1. Index laden ===
    base_url = "https://www.smard.de/app/chart_data"
    index_url = f"{base_url}/{FILTER_ID}/{REGION}/index_{RESOLUTION}.json"
    response = requests.get(index_url)
    response.raise_for_status()

    content = response.content
    if content[:2] == b'PK':
        with ZipFile(BytesIO(content)) as zip_file:
            json_filename = zip_file.namelist()[0]
            with zip_file.open(json_filename) as f:
                index_data = json.load(f)
    else:
        index_data = json.loads(content.decode("utf-8"))

    timestamps = sorted(int(ts) for ts in index_data.get("timestamps", index_data) if isinstance(ts, (int, float)))
    segment_list = [ts for ts in timestamps if start_ts <= ts <= end_ts]

    # === 2. Daten sammeln ===
    all_data = []
    unit = aggregation = None

    for seg_ts in segment_list:
        data_url = f"{base_url}/{FILTER_ID}/{REGION}/{FILTER_ID}_{REGION}_{RESOLUTION}_{seg_ts}.json"
        resp = requests.get(data_url)
        if not resp.ok:
            print(f"âš ï¸ Fehler bei Segment {seg_ts}: {resp.status_code}")
            continue

        content = resp.content
        if content[:2] == b'PK':
            with ZipFile(BytesIO(content)) as zip_file:
                json_filename = zip_file.namelist()[0]
                with zip_file.open(json_filename) as f:
                    raw_json = json.load(f)
        else:
            raw_json = json.loads(content.decode("utf-8"))

        if unit is None:
            meta = raw_json.get("meta", raw_json.get("data", {}))
            unit = meta.get("unit", "â‚¬/MWh")
            aggregation = meta.get("aggregation", RESOLUTION)

        series = raw_json.get("series", list(raw_json.values())[0])
        all_data.extend(series)

    # === 3. DataFrame vorbereiten ===
    df = pd.DataFrame(all_data, columns=["timestamp", "price_euro"])
    df = df[(df["timestamp"] >= start_ts) & (df["timestamp"] <= end_ts)].copy()
    df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms")
    df.set_index("timestamp", inplace=True)
    df["Unit"] = unit
    df["aggregation"] = aggregation
    df["Source"] = "Smard.de"
    df.reset_index(inplace=True)

    # === 4. Mit asyncpg verbinden ===
    conn = await asyncpg.connect(
        user=DB_USER,
        password=DB_PASS,
        database=DB_NAME,
        host=DB_HOST,
        port=DB_PORT
    )

    await conn.execute(f"SET search_path TO {DB_SCHEMA};")

    # Bestehende Timestamps abrufen
    rows = await conn.fetch(f"SELECT timestamp FROM {TABLE_NAME};")
    existing_timestamps = set(row["timestamp"] for row in rows)

    # Nur neue DatensÃ¤tze einfÃ¼gen
    new_rows = df[~df["timestamp"].isin(existing_timestamps)]

    if not new_rows.empty:
        values = [
            (row["timestamp"], row["price_euro"], row["Unit"], row["Source"], row["aggregation"])
            for _, row in new_rows.iterrows()
        ]

        await conn.executemany(
            f"""
            INSERT INTO {TABLE_NAME} (timestamp, price_euro, unit, source, aggregation)
            VALUES ($1, $2, $3, $4, $5)
            """,
            values
        )
        print(f"âœ… {len(values)} neue Zeilen gespeichert.")
    else:
        print("ðŸ” Keine neuen Daten gefunden.")

    await conn.close()

if __name__ == "__main__":
    asyncio.run(main())
